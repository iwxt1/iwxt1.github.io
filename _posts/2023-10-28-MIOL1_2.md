---
title: 'A Modern Introdution to Online Learning'
date: 2023-10-28
permalink: /posts/2023/10/MIOL1_2
excerpt_separator: <!--more-->
toc: true
tags:
  - Online Learning
---

A monograph on online learning by Francesco Orabona.
<!--more-->

# FTL

## Minimizing Regret

$$
\begin{equation*}
\mathcal{R}_T := \sum_{t=1}^T \ell_t(x_t) - \sum_{t=1}^T \ell_t(x^*_T)\;,
\end{equation*}
$$



where $$x^*_T = \arg\min_{x\in V} \sum_{t=1}^T \ell_t(x)$$.



In this section, the loss function $\ell_t(x) = (x-y_t)^2$.



Immediately we have



$$
\begin{equation*}
x_T^* := \arg\min_{x\in V} \sum_{t=1}^T (x-y_t)^2 = \frac{1}{T}\sum_{t=1}^T y_t\;.
\end{equation*}
$$



where $V$ is a convex set and $y_t\in V$.



However we cannot compute $$x_T^*$$ because we do not know $$y_t$$ in advance. So we cennot use $$x_T^*$$ as our guess in each round. Instead, we use $$x_{t-1}^*$$ as our guess in round $$t$$. Why it works? We should leverage the fact that the optimal guess over time cannot change too much between rounds. Hence, on each round $$t$$, we use the guess $$x_t = x_{t-1}^* = \frac{1}{t-1}\sum_{i=1}^{t-1}y_i$$. This is called **Follow the Leader** (FTL) algorithm.



## Lemma 1.1

Let $V\subseteq \mathbb{R}^d$ and $\ell_t:V \rightarrow \mathbb{R}$ and arbitrary sequence of loss functions. Denote by $x_t^*$ a minimizer of the cumulative loss over the previous $t$ rounds in $V$. Then, we have



$$
\begin{equation*}
\sum_{t=1}^T \ell_t(x_t^*) \leqslant \sum_{t=1}^T \ell_t(x_T^*)\;.
\end{equation*}
$$



Proof:



Induction over $T$. The base case is obvious.



$$
\begin{equation*}
\ell_1(x_1^*) \leqslant \ell_1(x_1^*)\;,
\end{equation*}
$$



Now assume that the statement is true for $T-1$. We have



$$
\begin{equation}
\sum_{t=1}^{T-1} \ell_t(x_t^*)  \leqslant \sum_{t=1}^{T-1} \ell_t(x_{T-1}^*)\;.
\end{equation}
$$



For the right side of (1), we have



$$
\begin{equation}
\sum_{t=1}^{T-1}\ell_t(x_{T-1}^*) \leqslant \sum_{t=1}^{T-1}\ell_t(x_T^*)\;,
\end{equation}
$$



because $$x_{T-1}^*$$ is a minimizer of the cumulative loss over the previous $$T-1$$ rounds. Combining (1) and (2), then add $$\ell_T(x_T^*)$$ to both sides, we have



$$
\begin{equation*}
\sum_{t=1}^T \ell_t(x_t^*) \leqslant \sum_{t=1}^T \ell_t(x_T^*)\;.
\end{equation*}
$$



So the lemma is proven.



## Theorem 1.2

Let $y_t \in \left[0,1\right]$ for $t = 1,\cdots, T$ an arbitrary sequence of numbers. Let the algorithm's output $x_t = x_{t-1}^* := \frac{1}{t-1}\sum_{i=1}^{t-1}y_i$. Then, we have



$$
\begin{equation*}
\mathcal{R}_T = \sum_{t=1}^T(x_t-y_t)^2 - \min_{x\in\left[0,1\right]}\sum_{t=1}^T(x-y_t)^2 \leqslant 4 + 4\ln T\;.
\end{equation*}
$$



Proof:



First use the Lemma 1.1 to upper bound the regret:



$$
\begin{align*}
\mathcal{R}_T &= \sum_{t=1}^T(x_t-y_t)^2 - \min_{x\in\left[0,1\right]}\sum_{t=1}^T(x-y_t)^2\\ &= \sum_{t=1}^T(x_{t-1}^*-y_t)^2 - \sum_{t=1}^T(x_T^*-y_t)^2 \\
&\leqslant \sum_{t=1}^T(x_{t-1}^*-y_t)^2 - \sum_{t=1}^T(x_t^*-y_t)^2 \\
\end{align*}
$$



Letâ€™s take a look at each difference in the sum in the last equation.



$$
\begin{align*}
(x_{t-1}^*-y_t)^2 - (x_t^*-y_t)^2 &= (x_{t-1}^*-x_t^*)(x_{t-1}^*+x_t^*-2y_t) \\
&\leqslant \left|x_{t-1}^*-x_t^*\right|\left|x_{t-1}^*+x_t^*-2y_t\right| \\
&\leqslant 2\left|x_{t-1}^*-x_t^*\right| \\
&= 2\left|\frac{1}{t-1}\sum_{i=1}^{t-1}y_i - \frac{1}{t}\sum_{i=1}^{t}y_i\right| \\
&= 2\left| \left( \frac{1}{t-1}-\frac{1}{t}\right)\sum_{i=1}^{t-1}y_i - \frac{y_t}{t} \right| \\
&= 2\left| \frac{1}{t(t-1)}\sum_{i=1}^{t-1}y_i - \frac{y_t}{t} \right| \\
&\leqslant 2\left| \frac{1}{t(t-1)}\sum_{i=1}^{t-1}y_i \right| + 2\left| \frac{y_t}{t} \right|\\
&\leqslant \frac{2}{t} + \frac{2}{t} = \frac{4}{t}\;.
\end{align*}
$$



So we have



$$
\begin{align*}
\mathcal{R}_T &\leqslant 4\sum_{t=1}^T\frac{1}{t}\\
&\leqslant 4\left(1 + \int_{1}^{T} \frac{1}{t}\,dt \right)\\
&= 4 + 4\ln T\;.\\
\end{align*}
$$



So the theorem is proven.



# Online Gradient Descent

![POGD](/images/posts/MIOL/POGD.png)



## Lemma 2.1

Let $x\in \mathbb{R}^d$ and $y\in V$, where $V\subseteq \mathbb{R}^d$ is a non-empty closed convex set and define $\Pi_V(x):= \arg\min_{y\in V}\|x-y\|_2$. Then, $\|\Pi_V(x) - y\|_2 \leqslant \|x-y\|_2$.



Proof:



First notice that $$\arg\min_{y\in V} \|x-y\|_2 = \arg\min_{y\in V} \frac{1}{2}\|x-y\|_2^2$$.



Let $f(y) = \frac{1}{2}\|x-y\|_2^2$. From the optimality condition, we have



$$
\begin{equation*}
\langle \nabla f(\Pi_V(x)), y - \Pi_V(x) \rangle = \langle \Pi_V(x) - x, y - \Pi_V(x) \rangle \geqslant 0\;.
\end{equation*}
$$



So we have



$$
\begin{align*}
\|x-y\|_2^2 &= \|x-\Pi_V(x) + \Pi_V(x) - y\|_2^2 \\
&= \|x-\Pi_V(x)\|_2^2 + \|\Pi_V(x) - y\|_2^2 + 2\langle x-\Pi_V(x), \Pi_V(x) - y \rangle \\
&\geqslant \|\Pi_V(x) - y\|_2^2\;.
\end{align*}
$$



So the lemma is proven.



## Lemma 2.2

Let $V\subseteq \mathbb{R}^d$ a non-empty closed convex set and $\ell_t:V\rightarrow \mathbb{R}$ a convex function differentiable in an open set that contains $V$. Set $g_t = \nabla \ell_t(x_t)$. Then, $\forall u \in V$, the following inequality holds



$$
\begin{equation}
\eta_t\left( \ell_t(x_t) - \ell_t(u) \right) \leqslant \eta_t\langle g_t, x_t-u\rangle \leqslant \frac{1}{2}\|x_t-u\|_2^2 - \frac{1}{2}\|x_{t+1}-u\|_2^2 + \frac{\eta_t^2}{2}\|g_t\|_2^2\;.
\end{equation}
$$



Proof:



The first inequality is trivial. With Lemma 2.1, then we have that



$$
\begin{align*}
\|x_{t+1} - u\|_2^2 - \|x_t - u\|_2^2 &= \|\Pi_V(x_t-\eta_t g_t) - u\|_2^2 - \|x_t - u\|_2^2 \\
&\leqslant \|x_t - \eta_t g_t - u\|_2^2 - \|x_t - u\|_2^2 \\
&= -2\eta_t\langle g_t, x_t - u \rangle + \eta_t^2\|g_t\|_2^2\\
&\leqslant -2\eta_t\left( \ell_t(x_t) - \ell_t(u) \right) + \eta_t^2\|g_t\|_2^2\;.
\end{align*}
$$



Reorganize the terms, we get the second inequality.



## Theorem 2.3

Let $V\subseteq \mathbb{R}^d$ a non-empty closed convex set with diameter $D$, i.e., $$\max_{x,y\in V}\|x-y\|_2 \leqslant D$$. Let $$\ell_1, \cdots, \ell_T$$ an arbitrary sequence of convex functions $$\ell_t:V\rightarrow \mathbb{R}$$ differentiable in open sets containing $$V$. Pick any $x_1 \in V$$ and assume $$\eta_{t+1} \leqslant \eta_t,\, t=1,\cdots, T$$. Then, $$\forall u \in V$$, the following regret bound holds



$$
\begin{equation*}
\sum_{t=1}^T\left(\ell_t(x_t)-\ell_t(u) \right) \leqslant \frac{D^2}{2\eta_T} + \sum_{t=1}^T\frac{\eta_t}{2}\|g_t\|_2^2\;.
\end{equation*}
$$



Moreover, if $\eta_t$ is constant, we have



$$
\begin{equation*}
\sum_{t=1}^T\left(\ell_t(x_t)-\ell_t(u) \right) \leqslant \frac{\|u-x_1\|_2^2}{2\eta} + \frac{\eta}{2}\sum_{t=1}^T\|g_t\|_2^2\;.
\end{equation*}
$$



Proof:



Dividing $\eta_t$ on both sides of (3), summing over $t=1,\cdots, T$, we have



$$
\begin{align*}
\sum_{t=1}^T \left(\ell_t(x)-\ell_t(u)\right) & \leqslant \frac{1}{2}\sum_{t=1}^T\frac{1}{\eta_t}\left( \|x_t-u\|_2^2 - \|x_{t+1}-u\|_2^2 \right) + \sum_{t=1}^T\frac{\eta_t}{2}\|g_t\|_2^2 \\
&= \frac{\|x_1-u\|_2^2}{2\eta_1} - \frac{\|x_{T+1}-u\|_2^2}{2\eta_T} + \frac{1}{2}\sum_{t=1}^{T-1}(\frac{1}{\eta_{t+1}} - \frac{1}{\eta_t})\|x_{t+1}-u\|_2^2 + \sum_{t=1}^T\frac{\eta_t}{2}\|g_t\|_2^2 \\
&\leqslant \frac{D^2}{2\eta_1} + \frac{D^2}{2}\sum_{t=1}^{T-1}(\frac{1}{\eta_{t+1}} - \frac{1}{\eta_t})+\sum_{t=1}^T\frac{\eta_t}{2}\|g_t\|_2^2\\
&= \frac{D^2}{2\eta_T} + \sum_{t=1}^T\frac{\eta_t}{2}\|g_t\|_2^2\;.
\end{align*}
$$



If $\eta_t$ is constant, then we have



$$
\begin{align*}
\sum_{t=1}^T \left(\ell_t(x)-\ell_t(u)\right) & \leqslant \frac{1}{2\eta}\sum_{t=1}^T\left( \|x_t-u\|_2^2 - \|x_{t+1}-u\|_2^2 \right) + \frac{\eta}{2}\sum_{t=1}^T\|g_t\|_2^2 \\
&= \frac{\|x_1-u\|_2^2}{2\eta} - \frac{\|x_{T+1}-u\|_2^2}{2\eta} + \frac{\eta}{2}\sum_{t=1}^T\|g_t\|_2^2 \\
&\leqslant \frac{\|x_1-u\|_2^2}{2\eta} + \frac{\eta}{2}\sum_{t=1}^T\|g_t\|_2^2\;. \tag{4}
\end{align*}
$$



So the theorem is proven.



We can immediately observe a few things.



- We need a bounded domain $V$ for the proof to work if we want to use time-varying learning rates.
- The regret bound helps us choosing the learning rate $\eta_t$.



Consider the constant learning rate $\eta$ in (4), assume that $\|g_t\|_2 \leqslant L$ for all $t$, with domain bound $D$. We have



$$
\begin{equation*}
\eta^* = \arg\min_\eta \frac{D^2}{2\eta} + \frac{\eta L^2T}{2} = \frac{D}{L\sqrt{T}}\;,
\end{equation*}
$$



that gives a regret bound of $DL\sqrt{T} = \mathcal{O}(\sqrt{T})$.



# Online Subgradient Descent

![POSGD](/images/posts/MIOL/POSGD.png)



## Theorem 3.1

If the function $f:\mathbb{R}^d\rightarrow [-\infty, +\infty]$ is convex and finite in $x$, it is differentiable in $x$ iff the subgradient is just the gradient $\nabla f(x)$.



## Theorem 3.2

Let $f_1, \cdots, f_m$ be proper functions on $\mathbb{R}^d$, and $f = \sum_{i=1}^m f_i$. Then, $\partial f(x) \supseteq \sum_{i=1}^m \partial f_i(x),\, \forall x$. Moreover, if $f_1,\cdots,f_m$ are also convex, closed, and $\mathrm{dom}\,f_m \cap \bigcap_{i=1}^{m-1}\mathrm{int}\,\mathrm{dom}\,f_i \neq \emptyset$, then actually $\partial f(x) = \sum_{i=1}^m \partial f_i(x),\, \forall x$.



Proof:



For any $z$, define $g_i \in \partial f_i(z)$ for $i = 1,\cdots, m$. Then, we have



$$
\begin{equation*}
f(x) = \sum_{i=1}^m f_i(x) \geqslant \sum_{i=1}^m \left( f_i(z) + \langle g_i, x-z \rangle \right) = f(z) + \langle \sum_{i=1}^m g_i, x-z \rangle\;.
\end{equation*}
$$



Hence, $\sum_{i=1}^m g_i \in \partial f(z)$. So $\partial f(x) \supseteq \sum_{i=1}^m \partial f_i(x),\, \forall x$.



**For the second part, I need learn something.**



## Theorem 3.3

Let $$(f_i)_{i\in I}$$ be a finite set of convex functions from $$\mathbb{R}^d$$ to $$(-\infty, +\infty]$$ and suppose $$x\in \bigcap_{i\in I} \mathrm{dom} f_i$$ and $$f_i$$ continuous at $$x$$. Set $$F = \max_{i\in I}f_i$$ and let $$A(x) = {i \in I \mid f_i(x) = F(x)}$$ the set of the active functions. Then



$$
\begin{equation*}
\partial F(x) = \mathrm{conv} \bigcap_{i\in A(x)} \partial f_i(x)\;.
\end{equation*}
$$



## Theorem 3.4

First define the Lipschitz Function. Let $f:\mathbb{R}^d \rightarrow (-\infty, +\infty]$ is **L-Lipschitz** over a set $V$ w.r.t a norm $\|\cdot\|$ if $\lvert f(x) - f(y) \rvert \leqslant L\|x-y\|, \forall x,y\in V$.



Let $f:\mathbb{R}^d\rightarrow (-\infty,+\infty]$ proper and convex. Then, $f$ is L-Lipschitz in $\mathrm{int}\,\mathrm{dom}f$ w.r.t the $L_2$ norm iff for all $x\in \mathrm{int}\,\mathrm{dom} f$ and $g\in \partial f(x)$ we have $\|g\|_2 \leqslant L$.



Proof:



$\Leftarrow$: Assume $f$ L-Lipschitz, then $\lvert f(x) - f(y)\rvert \leqslant L\|x-y\|_2, \forall x, y \in \mathrm{int}\,\mathrm{dom}f$. For $\epsilon > 0$ small enough $y = x + \epsilon \frac{g}{\|g\|_2}\in \mathrm{int}\,\mathrm{dom}f$, then 



$$
\begin{equation*}
L\epsilon = L\|x-y\|_2 \geqslant \lvert f(y) - f(x)\rvert \geqslant f(y) - f(x) \geqslant \langle g, y-x \rangle = \epsilon\|g\|_2\;.
\end{equation*}
$$



So $\|g\|_2 \leqslant L$.



$\Rightarrow$: Assume $\|g\|_2 \leqslant L$ for all $g\in \partial f(x)$, then 



$$
\begin{equation*}
f(x) - f(y) \leqslant \langle g, x-y \rangle \leqslant \|g\|_2\|x-y\|_2 \leqslant L\|x-y\|_2\;.
\end{equation*}
$$



The same for $g \in \partial f(y)$. So we get $\lvert f(x) - f(y)\rvert \leqslant L\|x-y\|_2$.



## Lemma 3.5

Let $V\subseteq \mathbb{R}^d$ a non-empty closed convex set and $\ell_t:V\rightarrow \mathbb{R}$ a convex function subdifferentiable in $V$. Set $g_t\in \partial \ell_t(x_t)$. Then, $\forall u\in V$, the following inequality holds



$$
\begin{equation*}
\eta_t(\ell_t(x_t) - \ell_t(u)) \leqslant \eta_t\langle g_t, x_t-u \rangle \leqslant \frac{1}{2}\|x_t-u\|_2^2 - \frac{1}{2}\|x_{t+1}-u\|_2^2 + \frac{\eta_t^2}{2}\|g_t\|_2^2\;.
\end{equation*}
$$



It is obvious, According to Lemma 2.2, just use subgradient instead of gradient(by definition, subgradient is a generalization of gradient). So the regret bound hold as well.



# Convex Losses and Linear Losses

Define the linear losses $\tilde{\ell}_t:=\langle g_t, x \rangle$, we have



$$
\begin{equation*}
\sum_{t=1}^T \ell_t(x_t) - \ell_t(u) \leqslant \sum_{t=1}^T \langle g_t, x_t - u \rangle = \sum_{t=1}^T \tilde{\ell}_t(x_t) - \tilde{\ell}_t(u)\;,
\end{equation*}
$$



which means we can upper bound the regret of the convex losses by the regret of the linear losses. It implies that we can build online algorithgms that deal only with linear losses, and then use them to solve the convex case seamlessly. However, this not imply that this reduction is always optimal.









