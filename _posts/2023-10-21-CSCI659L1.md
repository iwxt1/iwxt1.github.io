---
title: 'A introduction to online learning(L1)'
date: 2023-10-21
permalink: /posts/2023/10/CSCI659L1
excerpt_separator: <!--more-->
toc: true
tags:
  - online learning
  - CSCI659
---

The foundation and advances of the theory and algorithms of online learning/online convex optimization/sequential decision making, taught by Haipeng Luo.
<!--more-->



# Goal: Regret Minimization

$$
\begin{equation}
    \mathcal{R}_T \overset{\text{def}}{=} \sum_{t=1}^T f_t(w_t) - \min_{w \in \Omega} \sum_{t=1}^T f_t(w)\;.
\end{equation}
$$



$\Omega$ is a compact convex set. The above equation is also possible to have randomness from the player or the environment. Intuitively, the above equation measures how much we regret that we did not always choose the best $w$ in the previous rounds, so our total loss in $T$ rounds is larger than the best fixed $w$ by $\mathcal{R}_T$. But also, since we are comparing with a fixed $w$，$\mathcal{R}_T$ could also be negative.



If $\mathcal{R}_T = o(T)$，then we say that the algorithm is **sublinear regret**, which is also called **no regret**. In this case, $$\lim_{T\rightarrow \infty} \frac{\mathcal{R}}{T} = 0$$，which means that in average, our actions are as good as the best fixed action in hindsight. If $\mathcal{R}_T = \mathcal{O}(\sqrt{T})$，then we say that the algorithm is **square-root regret**. If $\mathcal{R}_T = \mathcal{O}(\log T)$，then we say that the algorithm is **logarithmic regret**.



# Online-to-Batch Reduction

## Theorem 1. 

![Alg1](/images/posts/CSCI659/Alg1OBR.png)



Suppose that $\mathcal{A}$ enjoys the following regret bound $\mathbb{E} \left[ \mathcal{R}_T \right] \leqslant B(T)$ for some function $B$. Then the generalization error of the output $\bar{w}$ of Online-to-Batch Reduction is bounded by



$$
\begin{equation}
\mathbb{E}\left[ L(\bar{w}) \right] \leqslant L(w^*) + \frac{B(T)}{T}
\end{equation}
$$



where $w^* \in \arg \min_{w \in \Omega} L(w)$.



### Option Ⅰ

Output: uniformly at random sample $\bar{t}$ from {1,$\cdots$, T}, then $\bar{w} = w_{\bar{t}}$.



Proof:



$$
\begin{align*}
\mathbb{E}\left[ L(\bar{w}) \right] &= \frac{1}{T}\sum_{t=1}^T\mathbb{E}\left[L(w_t)\right] = \frac{1}{T}\sum_{t=1}^T\mathbb{E}\left[\mathbb{E}_{z \sim \mathcal{D}} \ell(w_t, z) \right] \\
&= \frac{1}{T}\sum_{t=1}^T\mathbb{E}_{z \sim \mathcal{D}} \left[\ell(w_t, z) \right] = \mathbb{E}_{z\sim \mathcal{D}} \left[ \frac{1}{T}\sum_{t=1}^T \ell(w_t,z) \right] \\
& \leq \mathbb{E}_{z\sim \mathcal{D}} \left[ \min_{w \in \Omega} \frac{1}{T}\sum_{t=1}^T \ell(w,z) \right] + \frac{B(T)}{T} \\
&= \frac{1}{T}\sum_{t=1}^T \mathbb{E}_{z\sim \mathcal{D}} \left[ \ell(w^*,z) \right] + \frac{B(T)}{T} \\
&= \frac{1}{T}\sum_{t=1}^T L(w^*) + \frac{B(T)}{T} = L(w^*) + \frac{B(T)}{T}\;.
\end{align*}
$$



### Option Ⅱ

Output: $\bar{w} = \frac{1}{T}\sum_{t=1}^T w_t$ if $L(w)$ is convex in $w$. 



Proof:



$$
\begin{equation*}
L(\bar{w}) = L(\frac{1}{T}\sum_{t=1}^T w_t) \leq \frac{1}{T}\sum_{t=1}^T L(w_t)\;.
\end{equation*}
$$



Then we can use the same proof as Option Ⅰ.



Therefore, as long as $\mathcal{A}$ is no-regret, then the generalization error of $\bar{w}$ is arbitrarily close to the optimal error when the sample size $T$ is large enough.



This illustrates that online learning algorithms can be used for traditional statistical learning problems as well.



**Question**. Why making only one pass of the training set is important to Algorithm 1? Can you identify the step in the proof that will break as long as one training example is passed to A more than once?



**Answer**: making only one pass of the training set is important to Algorithm 1 because the proof of Theorem 1 is based on the fact that the training set is i.i.d. If we pass one training example more than once, then the training set is no longer i.i.d. and the proof will break.



# Hedge Algorithm

As is known to all, the FTL algorithm suffers linear regret in the worst case. In this section, we will introduce a simple algorithm called Hedge, which change the situation greatly that FTL suffers.



![Alg2](/images/posts/CSCI659/Alg2Hedge.png)



The regret for this algorithm can be written as:



$$
\begin{equation*}
\mathcal{R}_T = \sum_{t=1}^T \langle p_t, \ell_t \rangle - \min_{p \in \Delta(N)} \sum_{t=1}^T \langle p, \ell_t \rangle = \sum_{t=1}^T \langle p_t, \ell_t \rangle - \sum_{t=1}^T \ell_t(i^*)
\end{equation*}
$$



where $i^* = \arg \min_{i} \sum_{t=1}^T \ell_t(i)$.



## Theorem 2.

Hedge with learning rate $\eta$ guarantees for any sequence of loss vectors $\ell_1, \cdots, \ell_T \in \left[0,1\right]^N$:



$$
\begin{align}
\mathcal{R}_T &\leqslant \frac{\ln N}{\eta} + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\\
&\leqslant \frac{\ln N}{\eta} + T\eta\;,
\end{align}
$$



where $p_t$ is the distribution over $N$ experts at time $t$. $\mathcal{R}_T$ is of order $\mathcal{O}(\sqrt{T\ln N})$ when $\eta = \sqrt{\frac{\ln N}{T}}$.



Below will present a "potential-based" proof of Theorem 2. It uses bound(3) as a intermediate step, then get bound(4) immediately by the boundeness of losses.



Proof:



Define potential $\Phi_t = \frac{1}{\eta}\ln \left( \sum_{i=1}^N \exp(-\eta L_t(i)) \right)\;.$



$$
\begin{align*}
\Phi_t - \Phi_{t-1} &= \frac{1}{\eta}\ln \left( \sum_{i=1}^N \exp(-\eta L_t(i)) \right) - \frac{1}{\eta}\ln \left( \sum_{i=1}^N \exp(-\eta L_{t-1}(i)) \right) \\
&= \frac{1}{\eta}\ln \left( \frac{\sum_{i=1}^N \exp(-\eta L_t(i))}{\sum_{i=1}^N \exp(-\eta L_{t-1}(i))} \right)\\
& = \frac{1}{\eta}\ln \left( \frac{\sum_{i=1}^N \exp(-\eta \left(L_{t-1}(i)+ \ell_t(i) \right))}{\sum_{i=1}^N \exp(-\eta L_{t-1}(i))} \right)\\
&= \frac{1}{\eta}\ln \left( \frac{\sum_{i=1}^N \exp(-\eta L_{t-1}(i))\exp(-\eta \ell_t(i))}{\sum_{i=1}^N \exp(-\eta L_{t-1}(i))} \right)\\
&= \frac{1}{\eta}\ln \left( \sum_{i=1}^N p_t(i)\exp(-\eta \ell_t(i)) \right)\\
&\leqslant \frac{1}{\eta}\ln \left( \sum_{i=1}^N p_t(i)\left(1-\eta \ell_t(i) + \eta^2 \ell_t^2(i) \right) \right) \tag{$e^{-x} \leqslant 1-x+x^2$ for all $x \geqslant -1$}\\
&= \frac{1}{\eta}\ln \left( 1-\eta \langle p_t, \ell_t \rangle + \eta^2 \sum_{i=1}^N p_t(i)\ell_t^2(i) \right)\\
&\leqslant -\langle p_t, \ell_t \rangle + \eta \sum_{i=1}^Np_t(i)\ell_t^2(i)\;. \tag{$\ln(1+x) \leqslant x$}
\end{align*}
$$



summation over $t$, telescoping, and rearranging terms, we get



$$
\begin{align*}
\sum_{t=1}^T \langle p_t, \ell_t \rangle &\leqslant \Phi_0 - \Phi_T + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\\
&\leqslant \frac{\ln N}{\eta} - \frac{1}{\eta}\ln(\exp(-\eta L_T(i^*))) + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\\
&\leqslant \frac{\ln N}{\eta} + L_T(i^*) + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\;.
\end{align*}
$$



so we have bound (3) and (4)



$$
\begin{equation*}
\mathcal{R}_T = \sum_{t=1}^N \langle p_t, \ell_t \rangle  - L_T(i^*)\leqslant \frac{\ln N}{\eta} + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\;.
\end{equation*}
$$



and



$$
\begin{align*}
\mathcal{R}_T &\leqslant \frac{\ln N}{\eta} + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i)\ell_t^2(i)\\
&\leqslant \frac{\ln N}{\eta} + \eta \sum_{t=1}^T\sum_{i=1}^N p_t(i) \tag{ $\ell_t \in \left[0,1 \right]^N $} \\
&= \frac{\ln N}{\eta} + T\eta\;.
\end{align*}
$$



**Question**. Does Theorem 2 hold for an oblivious adversary or an adaptive adversary? If the learner randomly selects an expert $i_t$ according to $p_t$ at each time $t$, and we measure the regret by $\sum_{t=1}^T \ell_t(i_t) - \sum_{t=1}^T \ell_t(i^*)$, can we still obtain a similar regret bound (in expectation or with high probability)? If so, does it hold for an oblivious adversary or an adaptive adversary?



**Answer**:

It hold for an obvilious adversary. Because the proof of Theorem 2 does not use the fact that the adversary is adaptive. It only uses the fact that the loss is bounded by 1.






# Lower bound for the Expert Problem

In general, how can we tell whether a regret upper bound is satisfactory or not? The notion of minimax regret can be used to answer this question exactly. Intuitively, minimax regret is the smallest possible worst-case regret one can achieve. For example, the minimax regret of the expert problem with an oblivious adversary can be defined as



$$
\begin{equation*}
\min_{\mathcal{A}} \max_{\ell_1, \cdots, \ell_T} \mathbb{E}_{\mathcal{A}}\left[ \mathcal{R}_T \right]
\end{equation*}
$$



where $\mathcal{A}$ is any legitimate expert algorithm and $\mathbb{E}_{\mathcal{A}}$ highlights the randomness with respect to the
algorithm itself.



The existence of Hedge algorithm shows that



$$
\begin{equation*}
\min_{\mathcal{A}} \max_{\ell_1, \cdots, \ell_T} \mathbb{E}_{\mathcal{A}}\left[ \mathcal{R}_T \right] \leqslant 2\sqrt{T\ln N}
\end{equation*}
$$



If we can show a $\Omega(\sqrt{T\ln N})$ lower bound, then we call this bound the minimax optimal regret. Indeed, the following theorem shows that Hedge is minimax optimal.



## Theorem 3.

For any algorithm $\mathcal{A}$, we have



$$
\begin{equation*}
\inf_{T,N}\max_{\ell_1, \cdots, \ell_T} \frac{\mathbb{E}_{\mathcal{A}}\left[ \mathcal{R}_T \right]}{\sqrt{T\ln N}} \geqslant \frac{1}{\sqrt{2}}\;.
\end{equation*}
$$



Proof:



Let $\mathcal{D}$ be the uniform distribution over $\{0,1 \}$. We have



$$
\begin{align*}
\max_{\ell_1, \cdots, \ell_T} \mathbb{E}_{\mathcal{A}}\left[ \mathcal{R}_T \right] &\geqslant \mathbb{E}_{\ell_1, \cdots, \ell_T \overset{iid}{\sim}\mathcal{D}^N} \mathbb{E}_{\mathcal{A}}\left[ \mathcal{R}_T \right] \\
&= \sum_{t=1}^T \mathbb{E}_{\mathcal{A}, \ell_1,\cdots,\ell_t}\left[ \langle p_t, \ell_t \rangle \right] - \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \sum_{t=1}^T \mathbb{E}_{\mathcal{A}, \ell_1,\cdots,\ell_{t-1}}\langle p_t, \mathbb{E}_{\ell_t}\left[\ell_t\right] \rangle- \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \sum_{t=1}^T \mathbb{E}_{\mathcal{A}, \ell_1,\cdots,\ell_{t-1}}\langle p_t, \frac{1}{2}\mathbf{1} \rangle- \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \frac{1}{2}\sum_{t=1}^T \mathbb{E}_{\mathcal{A}, \ell_1,\cdots,\ell_{t-1}}\left[ \sum_{i=1}^N p_t(i) \right]- \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \frac{1}{2}\sum_{t=1}^T \mathbb{E}_{\mathcal{A}, \ell_1,\cdots,\ell_{t-1}}\left[ 1 \right]- \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \frac{T}{2} - \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \sum_{t=1}^T \ell_t(i^*) \right] \\
&= \frac{T}{2} - \mathbb{E}_{\ell_1,\cdots,\ell_T}\left[ \min_{i \in \left[N \right]} \sum_{t=1}^T \ell_t(i) \right] \\
&= \mathbb{E}_{\ell_1, \cdots, \ell_T}\left[ \max_{i\in \left[ N \right]} \sum_{t=1}^T(\frac{1}{2} - \ell_t(i)) \right]\\
&= \frac{1}{2}\mathbb{E}_{\sigma_1,\cdots, \sigma_T}\left[ \max_{i\in \left[ N \right]} \sum_{t=1}^T \sigma_t(i) \right]
\end{align*}
$$



where $\sigma_t(i)$ for $i \in \left[N\right], t\in \left[T\right]$ are i.i.d. Rademacher random variables.



Finally we have



$$
\begin{equation}
\lim_{T\rightarrow \infty}\lim_{N\rightarrow \infty} \frac{\mathbb{E}_{\sigma_1,\cdots,\sigma_T} \left[ \max_{i\in\left[N \right]}\sum_{t=1}^T \sigma_t(i) \right]}{\sqrt{T\ln N}} = \sqrt{2}\;.
\end{equation}
$$



This bound is asymptotic.



Proof of (5):



We can use the Lemma A.11 and A.12 from [Cesa-Bianchi and Lugosi, 2006](https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf).



Lemma A.11. Let $\{\sigma_t(i)\}$ be i.i.d. Rademacher random variables($i\in \left[N\right], t = 1,2,\cdots$), and let $G_1, \cdots, G_N$ be independent standard normal random variables. Then



$$
\begin{equation*}
\lim_{T\rightarrow \infty} \mathbb{E}\left[ \max_{i\in\left[N\right]} \frac{1}{\sqrt{T}} \sum_{t=1}^T \sigma_t(i) \right] = \mathbb{E}\left[\max_{i\in\left[N\right]}G_i\right]\;.
\end{equation*}
$$



Lemma A.12. Let $G_1, \cdots, G_N$ be independent standard normal random variables. Then



$$
\begin{equation*}
\lim_{N\rightarrow \infty}\frac{\mathbb{E}\left[\max_{i\in\left[N\right]}G_i\right]}{\sqrt{2\ln N}} = 1\;.
\end{equation*}
$$



Therefore, we have



$$
\begin{align*}
& \lim_{T\rightarrow \infty}\lim_{N\rightarrow \infty} \frac{\mathbb{E}_{\sigma_1,\cdots,\sigma_T} \left[ \max_{i\in\left[N \right]}\sum_{t=1}^T \sigma_t(i) \right]}{\sqrt{T\ln N}}\\
&= \lim_{N\rightarrow \infty}\frac{1}{\sqrt{\ln N}}\lim_{T\rightarrow \infty}\mathbb{E}_{\sigma_1,\cdots,\sigma_T} \left[ \max_{i\in\left[N \right]}\frac{1}{\sqrt{T}}\sum_{t=1}^T \sigma_t(i) \right]\\
&= \lim_{N\rightarrow \infty}\sqrt{2}\frac{\mathbb{E}\left[\max_{i\in\left[N\right]}G_i\right]}{\sqrt{2\ln N}} \\
&= \sqrt{2}\;.
\end{align*}
$$






















