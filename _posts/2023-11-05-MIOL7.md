---
title: 'A Modern Introdution to Online Learning 7'
date: 2023-11-05
permalink: /posts/2023/11/MIOL7
excerpt_separator: <!--more-->
toc: true
tags:
  - Online Learning
  - Follow the Regularized Leader
---

A monograph on online learning by Francesco Orabona.
<!--more-->

This blog will introduce a powerful and generic algorithm to do online convex optimization **FTRL**. First we will introduce the algorithm, then we get the upper bound of the regret in different settings. In the end, the optimistic FTRL will be introduced.

# The Follow-the-Regularized-Leader Algorithm
![FTRL](/images/posts/MIOL/FTRL.png)
At each time step FTRL will play the minimizer of the sum of the past losses plus a time-varying regularization. It is clear that it is a family of algorithms.

From the algorithm, we can see that FTRL keeps in memory the entire history of the past, that in principle allows to recover the iterates before the projection in   $ V $.

In order to get the regret bound of FTRL. First we prove an equality for the regret. It factors the regret in three terms that have precise meanings and can be easily upper bounded with some familiar quantities. 

## Lemma 1.1
Let     $ V\subseteq \mathbb{R}^d $     be closed and non-empty. Denote by  $ F_t(x)=\psi_t(x)+\sum_{i=1}^{t-1}\ell_i(x) $. Assume that   $ \arg\min_{x\in V}F_t(x) $      is not empty and set   $ x_t\in \arg\min_{x\in V}F_t(x) $. Then, for any     $ u\in \mathbb{R}^d $, we have

$$
\begin{equation*}
\sum_{t=1}^T\left(\ell_t(x_t)-\ell_t(u) \right) = \psi_{T+1}(u)-\min_{x\in V}\psi_1(x)+\sum_{t=1}^T \left[F_t(x_t)-F_{t+1}(x_{t+1})+\ell_t(x_t) \right] + F_{T+1}(x_{T+1}) - F_{T+1}(u)\;.
\end{equation*}
$$

Proof:

Remove the      $ \ell_t(u) $   term from both sides and remember that      $ F_1(x_1) = \min_{x\in V} \psi_1(x) $, telescoping, then we can get the result.

We need to notice something:
- We didn't assume anything on    $  \ell_t $    nor on    $ \psi_t $, so the above equality holds even for non-convex losses and regularizers.
- We will often set   $ \psi_{T+1} $    equal to    $ \psi_T $, because the left hand side of the equality does not depend on     $ \psi_{T+1} $.
- The FTRL algorithm is invariant to any constant added to the regularizers, i.e. we can state the regret guarantee with    $ \psi_t(u)-\min_x\psi_t(x) $   instead of    $ \psi_t(u) $
- We can write the same equality for the prediction of a generic algorithm whose regret will depend on the prediction of the FTRL because the term    $ \ell_t(x_t) $   appear on the left and right of the equality. We can just subsitute the term.

However, the regret of the above equality is implicit. In the following, we will see different ways to get an explicit regret upper bound from Lemma 1.1.


## FTRL Regret Bound using Strong Convexity
### Lemma 1.2
Let   $ f:\mathbb{R}^d\rightarrow (-\infty,+\infty] $   be proper and   $ \mu $-strongly convex w.r.t a norm  $ \|\cdot\| $   over a convex set   $ V\subseteq \mathrm{dom}\,\partial f $, where  $ \mu > 0 $. Then, for all  $ x,y\in V,g_y\in \partial f(y) $,  and   $ g_x\in\partial f(x) $,    we have

$$
\begin{equation*}
f(x)-f(y) \le \langle g_y, x-y \rangle + \frac{1}{2\mu}\|g_y-g_x\|_*^2\;.
\end{equation*}
$$

Proof:

Define    $ \phi(z) = f(z) - \langle g_y, z \rangle $   is    $ \mu $-strongly convex. And    $ \partial \phi(z) = \partial f(z) - g_y $. We can see that   $ 0\in \partial \phi(y) $, so   $ y $   is a minimizer of  $ \phi $. Also, note that   $ g_x - g_y \in \partial \phi(x) $, so we have

$$
\begin{align*}
\phi(y)
&= \min_{z\in\mathrm{dom}\,\phi}\phi(z)\\
&\ge \min_{z\in\mathrm{dom}\,\phi}\left(\phi(x) + \langle g_x-g_y, z-x\rangle + \frac{\mu}{2}\|z-x\|^2 \right)\\
&\ge \min_{z\in \mathbb{R}^d}\left(\phi(x) + \langle g_x-g_y, z-x\rangle + \frac{\mu}{2}\|z-x\|^2 \right)\\
&= \phi(x) -\mu\max_{z\in\mathbb{R}^d}\left(\langle \frac{g_y-g_x}{\mu}, z-x\rangle - \frac{1}{2}\|z-x\|^2 \right)\\
&= \phi(x) - \frac{1}{2\mu}\|g_y-g_x\|_*^2\;.
\end{align*}
$$

Substitute   $ \phi  $     and rearrange the terms, we can get the result.

Use the above lemma, we can get a corollary.


### Corollary 1.3
Let   $ f:\mathbb{R}^d\rightarrow (-\infty, +\infty] $    closed, proper, subdifferentiable, and    $ \mu $-strongly convex w.r.t a norm  $ \|\cdot\| $   over its domain. Let    $ x^* = \arg\min_x f(x) $. Then, for all  $ x\in \mathrm{dom}\,\partial f $, and   $g\in\partial f(x)$, we have

$$
\begin{equation*}
f(x)-f(x^*) \le \frac{1}{2\mu}\|g\|_*^2\;.
\end{equation*}
$$

The corollary says that an upper bound to the suboptimality gap is proportional to the squared norm of the subgradient. We can use this corollary to get the regret bound of FTRL.

### Lemma 1.4
Assume    $ V\subseteq X $    and   $ V $   convex. If   $ F_t $   is closed, subdifferentiable, and strongly convex in   $ V $, then    $ x_t $   exists and is unique. In addtion, assume    $ \partial \ell_t(x_t) $   to be non-empty and    $ F_t + \ell_t $  to be closed, subdifferentiable, and    $ \lambda_t $-strongly convex w.r.t    $ \|\cdot\| $   in   $ V $. Then, we have

$$
\begin{equation*}
F_t(x_t)-F_{t+1}(x_{t+1}) + \ell_t(x_t)\le \frac{\|g_t\|_*^2}{2\lambda_t} + \psi_t(x_{t+1}) - \psi_{t+1}(x_{t+1})\;.
\end{equation*}
$$

Proof:
The existence and unicity can be proven by the Weierstrass Theorem for extended functions and the fact that strongly convex function are strictly convex. Then, we have

$$
\begin{align*}
F_t(x_t)-F_{t+1}(x_{t+1}) + \ell_t(x_t)
&= (F_t(x_t) + \ell_t(x_t)) - (F_t(x_{t+1}) + \ell_t(x_{t+1})) + \psi_t(x_{t+1}) - \psi_{t+1}(x_{t+1})\\
&\le (F_t(x_t) + \ell_t(x_t)) - (F_t(x^*) + \ell_t(x^*)) + \psi_t(x_{t+1}) - \psi_{t+1}(x_{t+1})\\
&\le \frac{\|g_t\|_*^2}{2\lambda_t} + \psi_t(x_{t+1}) - \psi_{t+1}(x_{t+1})\;,
\end{align*}
$$

where   $ x_t^*:= \arg\min_{x\in V}F_t(x)+\ell_t(x) $, and    $ g_t\in \partial(F_t+\ell_t+i_V)(x_t) $. Observing that    $ x_t = \arg\min_{x\in V} F_t(x) $, we have   $ 0\in \partial(F_t + i_V)(x_t) $. Hence, we have    $ \partial \ell_t(x_t)\subseteq \partial(F_t + \ell_t + i_V)(x_t) $.

Then we can get a upper bound of FTRL.


### Corollary 1.5
Let    $ V $    be convex and let   $ \psi:V\rightarrow \mathbb{R} $   be a closed and    $ \mu $-strongly convex function w.r.t.    $ \|\cdot\| $. Set the sequence of regularizers as  $ \psi_t(x) = \frac{1}{\eta_{t-1}}(\psi(x)-\min_z\psi(z)) $, where   $ \eta_{t+1} \le \eta_t, t=1,\cdots,T  $. Assume    $ \ell_t $    convex, closed, and     $ \partial \ell_t(x_t) $    not empty. Then, FTRL guarantees

$$
\begin{equation*}
\sum_{t=1}^T\ell_t(x_t)-\sum_{t=1}^T\ell_t(u)\le \frac{\psi(u)-\min_{x\in V}\psi(x)}{\eta_{T-1}} + \frac{1}{2\mu}\sum_{t=1}^T\eta_{t-1}\|g_t\|_*^2\;,
\end{equation*}
$$

for all    $ g_t\in \partial \ell_t(x_t) $. Moreover, if the functions   $ \ell_t $    are    $ L $-Lipschitz, setting     $ \eta_{t-1} = \frac{\alpha \sqrt{\mu}}{L\sqrt{t}} $, we get

$$
\begin{equation*}
\sum_{t=1}^T\ell_t(x_t)-\sum_{t=1}^T\ell_t(u)\le \left(\frac{\psi(u)-\min_{x\in V}\psi(x)}{\alpha} + \alpha\right)\frac{L\sqrt{T}}{\sqrt{\mu}}\;.
\end{equation*}
$$

Proof:

We can use Lamma 1.1 and 1.4, observing that we have     $ \psi_t(x) - \psi_{t+1}(x)\le 0,\forall x $, and set   $ \psi_{T+1} = \psi_T $.


## FTRL with Linearized Losses
### Theorem 1.6
Let    $ f:\mathbb{R}^d\rightarrow (-\infty,+\infty] $    be proper. Then, the following conditions are equivalent:

(a) $ \theta\in\partial f(x) $.

(b) $ \langle \theta,y\rangle - f(y) $    achieves its supremum in   $ y $   at   $ y = x $.

(c) $ f(x) + f^*(\theta) = \langle \theta, x \rangle $.

More over, if   $ f $   is convex and closed, we have an additional equivalent condition:

(d) $ x\in \partial f^*(\theta) $.

Proof:

$ (a)\Leftrightarrow (b) $     use the definition of subgradient.

$ (b)\Leftrightarrow (c) $     use the definition of Fenchel conjugate.

If     $ f $    is also convex and closed, then    $ f^{**} = f $    is proper. Hence, (c) is equivalent to    $ f^{**}(x) + f^*(\theta) = \langle \theta, x\rangle $, i.e.   $ \langle x, y \rangle - f^*(y) $         achieves its supremum in   $ y $   at   $ y = \theta $, so  $ x\in \partial f^*(\theta) $. Hence, (c) is equivalent to (d).


### Theorem 1.7
(Duality Strong Convexity/Smoothness). Let     $ f:\mathbb{R}^d\rightarrow (-\infty,+\infty] $    be a proper, closed, convex function, and     $ \mathrm{dom}\partial f $    be non-empty. Then,     $ f\text{ is } \lambda > 0 $     strongly convex w.r.t.    $ \|\cdot\| $    if and only if     $ f^* $    is     $ \frac{1}{\lambda} $-smooth w.r.t.    $ \|\cdot\|_* $ on     $ \mathbb{R}^d$.


### Regret Bound
Consider the losses are linear, and the prediction of FTRL is 

$$
\begin{equation*}
x_{t+1}\in\arg\min_{x\in V}\psi_{t+1}(x) + \sum_{i=1}^t\langle g_i,x\rangle = \arg\max_{x\in V}\left\langle -\sum_{i=1}^t g_i, x\right\rangle - \psi_{t+1}(x)\;.
\end{equation*}
$$

Denote by     $ \psi_{V,t}(x) = \psi_t(x) + i_V(x) $. Now, if we assume   $ \psi_{V,t} $     to be proper, convex, and closed, using Theorem 1.6, we have that      $ x_{t+1} \in \partial\psi _{V,t+1}^* (-\sum_{i=1}^t g_i) $. Moreover, if     $ \psi _{V,t+1} $     is also strongly convex, by Theorem 1.7 we know that    $ \psi _{V,t+1}^* $     is differentiable and we get

$$
\begin{equation}
x_{t+1} = \nabla\psi_{V,t+1}^*(-\sum_{i=1}^tg_i)\;.
\end{equation}
$$

In turn, this update can be written in the following way:

$$
\begin{align*}
  \theta_{t+1} &= \theta_t - g_t\,\\
  x_{t+1} &= \nabla\psi_{V,t+1}^*(\theta_{t+1})\;.
\end{align*}
$$

We can immediately see:
- the state is kept directly in the dual space, updated and then transformed in the primal variable. The primal variable is only used to predict, but not directly in the update.

![FTRL with Linearized Losses](/images/posts/MIOL/FTRLLL.png)

Using the definition of the subgradients and the assumptions of Corollary 1.5, we have

$$
\begin{equation*}
\mathcal{R}_T(u)=\sum_{t=1}^T(\ell_t(x_t)-\ell_t(u))\le \sum_{t=1}^T(\tilde{\ell_t}(x_t)-\tilde{\ell_t}(u))\le \frac{\psi(u)-\min_{x\in V}\psi(x)}{\eta_{T-1}} + \frac{1}{2\mu}\sum_{t=1}^T\eta_{t-1}\|g_t\|_*^2\;,\forall u\in V\;,
\end{equation*}
$$

where  $ \tilde{\ell_t}(x) = \langle g_t, x\rangle $.


### Lemma 1.8
Under the assumptions of Lemma 1.1, further assume for all    $ t $   that   $ \ell_t(x) = \langle g_t,x\rangle $. Then, for any    $ u\in \mathbb{R}^d $, we have

$$
\begin{align*}
\sum_{t=1}^T\langle g_t,x_t-u\rangle &= \psi_{T+1}(u)-\min_{x\in V}\psi_1(x) + \sum_{t=1}^T\left[\psi_{V,t+1}^*\left(-\sum_{i=1}^tg_i \right) - \psi_{V,t}^*\left(-\sum_{i=1}^{t-1}g_i \right) +\langle g_t,x_t\rangle \right]\\
&+F_{T+1}(x_{T+1}) - F_{T+1}(u)\;,
\end{align*}
$$

where   $ \psi_{V,t}(x) = \psi_t(x) + i_V(x) $.

Proof:

Use Lemma 1.1 and observing that

$$
\begin{equation*}
F_t(x_t) = \min_{x\in V}\psi_t(x) + \sum_{i=1}^{t-1}\ell_i(x) = -\max_{x\in V}\left\langle -\sum_{i=1}^{t-1}g_i,x\right\rangle-\psi_t(x) = -\psi_{V,t}^*\left(-\sum_{i=1}^{t-1}g_i \right)\;.
\end{equation*}
$$

we can immediately get the result.


## FTRL Regret Bound using Local Norms
### Lemma 1.9
Under the same assumptions of Lemma 1.1, assume     $ \psi_1,\ldots,\psi_T $    twice differentiable and with the Hessian positive definite in the interior of their domains, and     $ \psi_{t+1} \ge \psi_t(x),\forall x\in V, t=1,\ldots,T $. Also, assume     $ \ell_t(x) = \langle g_t, x\rangle, t=1,\ldots,T $, for arbitrary     $ g_t $. Assume that    $ x_t\in\mathrm{int}\,\mathrm{dom}\,\psi_t $    and that       $     \tilde{x}_{t+1} := \arg\min _{ \in \mathbb{R}^d } \langle g_t, x \rangle + B_{\psi_t} (x; x_t) $      exist. Then, there    $ z_t $    on the line segments between     $ x_t $    and    $ x_{t+1} $      and   $ \tilde{z}_{t} $    on the line segment between     $ x_{t} $    and    $ \tilde{x}_{t+1} $    such that the following inequalities hold for any    $ u\in V $:

$$
\begin{equation*}
\sum_{t=1}^T\langle g_t,x_t-u\rangle\le\psi_{T+1}(u) - \min_{x\in V}\psi_1(x)+\frac{1}{2}\sum_{t=1}^{T}\min\left(\|g_t\|^2_{(\nabla^2\psi_t(z_t))^{-1}},\|g_t\|^2_{(\nabla^2\psi_t(z_t'))^{-1}} \right)\;.
\end{equation*}
$$

Proof:

From the optimality condition of    $ x_t $, we have

$$
\begin{equation*}
\langle \nabla F_t(x_t), x_{t+1} - x_t\rangle \ge 0\;,
\end{equation*}
$$

Then, the Bregman divergence

$$
\begin{equation*}
B_{F_t}(x_{t+1};x_t) = F_t(x_{t+1}) - F_t(x_t) - \langle \nabla F_t(x_t), x_{t+1} - x_t\rangle \le F_t(x_{t+1}) - F_t(x_t)\;.
\end{equation*}
$$

Then, we have

$$
\begin{align*}
F_t(x_t) - F_{t+1}(x_{t+1}) + \ell_t(x_t) &= F_t(x_t)-F_t(x_{t+1})+\ell_t(x_t)-\ell_t(x_{t+1})+\psi_t(x_{t+1})-\psi_{t+1}(x_{t+1})\\
&\le F_t(x_t)-F_t(x_{t+1})+\ell_t(x_t)-\ell_t(x_{t+1})\\
&= F_t(x_t)-F_t(x_{t+1})+\langle g_t,x_t-x_{t+1}\rangle\\
&\le \langle g_t,x_t-x_{t+1}\rangle - B_{F_t}(x_{t+1};x_t)\\
&= \langle g_t,x_t-x_{t+1}\rangle - B_{\psi_t}(x_{t+1};x_t)\\
&\le \frac{1}{2}\|g_t\|^2_{(\nabla^2\psi_t(z_t))^{-1}} + \frac{1}{2}(x_{t+1}-x_t)^{\top}\nabla^2\psi_t(z_t)(x_{t+1}-x_t) - B_{\psi_t}(x_{t+1};x_t)\\
&= \frac{1}{2}\|g_t\|^2_{(\nabla^2\psi_t(z_t))^{-1}}\;,
\end{align*}
$$

where    $ B_{\psi_t}(x_{t+1};x_t) = \frac{1}{2}(x_{t+1}-x_t)^{\top}\nabla^2\psi_t(z_t)(x_{t+1}-x_t) $    from the Taylor's theorem. Then we get the first term in the minimum. For the second term, we have

$$
\begin{align*}
F_t(x_t) - F_{t+1}(x_{t+1}) + \ell_t(x_t)
&\le \langle g_t,x_t-x_{t+1}\rangle - B_{\psi_t}(x_{t+1};x_t)\\
&\le \max_{x\in\mathbb{R}^d}\langle g_t,x_t-x\rangle - B_{\psi_t}(x;x_t)\\
&= \langle g_t,x_t-\tilde{x}_{t+1}\rangle - B_{\psi_t}(\tilde{x}_{t+1};x_t)\;.
\end{align*}
$$

Then, we proceed as in the first bound.


## Optimistic FTRL
![Optimistic FTRL](/images/posts/MIOL/OpFTRL.png)
In Optimistic FTRL, we predict the next loss function. If our predicted loss is correct, we can expect the regret to decrease. And if our prediction is wrong we still want to recover the worst case guarantee. Such algorithm is called Optimistic FTRL.    

Let's assume for a moment that instead we have the gift of predicting the future, so we do know the next loss ahead of time. Then, we could predict with its minimizer and suffer a negative regret. However, probably our foresight abilities are not so powerful, so our prediction of the next loss might be inaccurate. In this case, a better idea might be just to add our predicted loss to the previous ones and minimize the regularized sum. We would expect the regret guarantee to improve if our prediction of the future loss is precise. At the same time, if the predictio is wrong, we expect its influence to be limited. All these intuitions can be formalized in the following Theorem.

### Theorem 1.10
With the notation in Algorithm 7.8, let     $ V $    be convex, closed, non-empty. Denote by     $ F_t(x) = \psi_t(x) + \sum_{i=1}^{t-1}\ell_i(x),t=1,\ldots,T $, if    $ F_t $    is closed, subdifferentiable, and strongly convex in     $ V $, then     $ x_t $    exists and is unique. In addition, for    $ t = 1,\ldots,T $, assume     $ \partial (\ell_t-\tilde{\ell}_t)(x_t) $    to be non-empty and     $ F_t + \ell_t $    to be closed, subdifferentiable, and     $ \lambda_t $-strongly convex w.r.t.     $ \|\cdot\| $    in     $ V $. Then, we have

$$
\begin{align*}
&\sum_{t=1}^T\ell_t(x_t)-\sum_{t=1}^T\ell_t(u)\\
&\le \psi_{T+1}(u) - \psi_1(x_1)+\sum_{t=1}^T\left[\langle \hat{g}_t,x_t-x_{t+1}\rangle - \frac{\lambda_t}{2}\|x_t-x_{t+1}\|^2+\psi_t(x_{t+1})-\psi_{t+1}(x_{t+1}) \right]\\
&\le \psi_{T+1}(u) - \psi_1(x_1)+\sum_{t=1}^T\left[\frac{\|\hat{g}_t\|_*^2}{2\lambda_t}+\psi_t(x_{t+1})-\psi_{t+1}(x_{t+1}) \right]\;,
\end{align*}
$$

for all    $ u\in V $    and all     $ \hat{g}_t \in \partial (\ell_t-\tilde{\ell}_t)(x_t),t=1,\ldots,T $.

Proof:

Set 
$ \tilde{\psi}_t(x) = \psi_t(x) + \tilde{\ell}_t(x) $. 
So, as in Lemma 1.4, the existence and unicity of    $ x_t $    can be given immediately. Also, note that     
$ \tilde{\ell}_{T+1}(x) $    
has no influence on the regret of the algorithm over the    
$ T $    
rounds, so we can set it to the null function. For all    
$ u \in V $, 
use Lemma 1.1 and discard the last term because negative, we have

$$
\begin{equation*}
  \sum_{t=1}^T\ell_t(x_t)-\sum_{t=1}^T\ell_t(u)\le \psi_{T+1}(u)-\psi_1(x_1) + \sum_{t=1}^T\left[F_t(x_t)-F_{t+1}(x_{t+1})+\ell_t(x_t) \right]\;.
\end{equation*}
$$

Observe that    $ F_t(x)+\ell_t(x)+i_V(x) $    is    $ \lambda_t $-strongly convex w.r.t.    $ \|\cdot\| $, we have

$$
\begin{align*}
&F_t(x_t)-F_{t+1}(x_{t+1})+\ell_t(x_t)\\
&= (F_t(x_t)+\ell_t(x_t))-(F_t(x_{t+1})+\ell_t(x_{t+1}))+\psi_t(x_{t+1})-\psi_{t+1}(x_{t+1})\\
&\le \langle \hat{g}_t,x_t-x_{t+1}\rangle - \frac{\lambda_t}{2}\|x_t-x_{t+1}\|^2 + \psi_t(x_{t+1})-\psi_{t+1}(x_{t+1})\;,
\end{align*}
$$

for all  
$ \hat{g}_t \in \partial (F_t+\ell_t+i_V)(x_t) $. 
And     
$ x_t = \argmin_{x\in V}F_t(x)+\tilde{\ell}_t(x) $, 
so   
$ 0\in \partial(F_t+\tilde{\ell}_t+i_V)(x_t) $. 
Hence,     
$ \partial (\ell_t-\tilde{\ell}_t )(x_t) \subseteq \partial(F_t+\ell_t+i_V)(x_t) $.

For the second inequality, by the definition of dual norms, we have that

$$
\begin{equation*}
\langle \hat{g}_t,x_t-x_{t+1}\rangle - \frac{\lambda_t}{2}\|x_t-x_{t+1}\|^2 \le \|\hat{g}_t\|_*\|x_t-x_{t+1}\| - \frac{\lambda_t}{2}\|x_t-x_{t+1}\|^2 \le \frac{\|\hat{g}_t\|_*^2}{2\lambda_t}\;,
\end{equation*}
$$

So we can get the result.

Let's take a look at the second bound in the theorem. Compared to the similar bound for FTRL, we now have the terms    
$ \|\hat{g}_t\|_*^2 $    
instead of the ones    
$ \|g_t\|_*^2 $. 
Let     
$ \tilde{\ell}_t(x) = \langle \tilde{g}_t,x\rangle $, 
then the bound depends on     
$ \|g_t-\tilde{g}_t\|_*^2 $. 
This means that if the prediction of the subgradient of the next loss is good, then that term can be small and even zero. Even if the predictions are bad, for Lipschitz losses we only lose a constant factor.